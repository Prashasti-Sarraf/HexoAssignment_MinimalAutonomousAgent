# -*- coding: utf-8 -*-
"""minimal_autonomous_agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q65F4NF3sTSzX6heFcJNximkLLRFgrEa
"""

"""
Minimal Autonomous Agent for MLEbench-lite datasets

ONE-LINER:
python minimal_autonomous_agent.py --dataset_dir <path> --output submission.csv --seeds 0 1 2 --time_limit 86400
"""

import os
import json
import time
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# -----------------------------
# Sklearn models
# -----------------------------
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split

# -----------------------------
# Optional: for image and timeseries pipelines
# -----------------------------
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torchvision.transforms as transforms
    import torchvision.models as models
    from torch.utils.data import DataLoader, Dataset
    from PIL import Image
except ImportError:
    torch = None
    print("Torch not installed. Image/Time-series pipelines will be disabled.")


from pathlib import Path
import pandas as pd
import numpy as np

# -----------------------------
# Logging helper
# -----------------------------
def log(msg, logfile):
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    line = f"[{ts}] {msg}"
    print(line)
    with open(logfile, "a") as f:
        f.write(line + "\n")

# -----------------------------
# Column detection for text/tabular
# -----------------------------
def get_text_and_label_cols(df):
    if "text" in df.columns and "author" in df.columns:
        return "text", "author"

    object_cols = [c for c in df.columns if df[c].dtype == "object"]
    if object_cols:
        text_col = max(object_cols, key=lambda c: df[c].astype(str).str.len().mean())
    else:
        text_col = df.columns[0]

    for c in df.columns:
        if c.lower() in ("label", "target", "class"):
            return text_col, c

    candidate = None
    for c in df.columns:
        if c != text_col and df[c].nunique() <= 10:
            candidate = c
            break

    if candidate is None:
        candidate = df.columns[-1]

    return text_col, candidate

# -----------------------------
# Modality detection
# -----------------------------

def detect_modality(dataset_dir):
    p = Path(dataset_dir)
    csvs = list(p.glob("*.csv")) + list(p.rglob("*.csv"))

    if csvs:
        df = pd.read_csv(csvs[0], nrows=200)

        text_like_cols = []
        for col in df.columns:
            if df[col].dtype == object:
                # fraction of entries that are non-numeric strings
                non_num_frac = df[col].apply(
                    lambda x: isinstance(x, str) and not x.replace('.', '', 1).isdigit()
                ).mean()

                # average length of strings
                avg_len = df[col].astype(str).apply(len).mean()

                # threshold heuristic for text-like column
                if non_num_frac > 0.7 and avg_len > 20:
                    text_like_cols.append(col)

        if len(text_like_cols) > 0:
            return "text"
        else:
            return "tabular"

    # Images
    image_files = list(p.glob("**/*.png")) + list(p.glob("**/*.jpg"))
    if image_files:
        return "image"

    # Time series (npy / tsv)
    ts_files = list(p.glob("**/*.npy")) + list(p.glob("**/*.tsv"))
    if ts_files:
        return "timeseries"

    return "unknown"

# -----------------------------
# Text pipeline
# -----------------------------
def run_text(dataset_dir, output_path, seed, logfile):
    p = Path(dataset_dir)
    train_csv = next((f for f in p.glob("train.csv")), None)
    test_csv = next((f for f in p.glob("test.csv")), None)

    if train_csv is None:
        log("No train.csv found", logfile)
        return None

    log(f"Reading train.csv:{train_csv}", logfile)
    df = pd.read_csv(train_csv)
    log(f"First 5 rows:{df.head()}", logfile)

    text_col, label_col = get_text_and_label_cols(df)
    log(f"Using text_col={text_col}, label_col={label_col}", logfile)

    y = df[label_col]
    X = df[text_col].astype(str)

    if y.nunique() < 2:
        log("Dataset contains <2 classes. Skipping.", logfile)
        return None

    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, random_state=seed, stratify=y
    )

    vectorizer = TfidfVectorizer(max_features=20000, ngram_range=(1, 2))
    X_train_vec = vectorizer.fit_transform(X_train)
    X_val_vec = vectorizer.transform(X_val)

    clf = LogisticRegression(max_iter=200, n_jobs=-1)
    clf.fit(X_train_vec, y_train)

    pred = clf.predict(X_val_vec)
    acc = accuracy_score(y_val, pred)
    log(f"Seed {seed}: val accuracy = {acc:.4f}", logfile)

    if seed == 0 and test_csv is not None:
        test_df = pd.read_csv(test_csv)
        test_X = test_df[text_col].astype(str)
        test_vec = vectorizer.transform(test_X)
        test_pred = clf.predict(test_vec)

        sub = pd.DataFrame({
            "id": test_df["id"] if "id" in test_df.columns else np.arange(len(test_df)),
            label_col: test_pred
        })
        sub.to_csv(output_path, index=False)
        log(f"Wrote submission.csv ({len(sub)} rows)", logfile)

    return acc

# -----------------------------
# Tabular pipeline
# -----------------------------

def run_tabular(dataset_dir, output_path, seed, logfile):
    import pandas as pd
    import numpy as np
    from pathlib import Path
    from sklearn.model_selection import train_test_split
    from sklearn.impute import SimpleImputer
    from lightgbm import LGBMClassifier

    p = Path(dataset_dir)

    # -----------------------------------------------------
    # Load train.csv
    # -----------------------------------------------------
    train_csv = p / "train.csv"
    if not train_csv.exists():
        log("No train.csv found", logfile)
        return None

    df = pd.read_csv(train_csv)
    log(f"Loaded train.csv shape = {df.shape}", logfile)

    # -----------------------------------------------------
    # Detect label column
    # -----------------------------------------------------
    label_candidates = ["target", "label", "class", "y"]
    label_col = next((c for c in label_candidates if c in df.columns), df.columns[-1])

    X = df.drop(columns=[label_col])
    y = df[label_col]

    # -----------------------------------------------------
    # Try numeric conversion for object columns
    # -----------------------------------------------------
    for col in X.columns:
        if X[col].dtype == "object":
            conv = pd.to_numeric(X[col], errors="coerce")
            if conv.notna().mean() > 0.9:
                X[col] = conv

    # -----------------------------------------------------
    # Column type split
    # -----------------------------------------------------
    numeric_cols = [c for c in X.columns if np.issubdtype(X[c].dtype, np.number)]
    categorical_cols = [c for c in X.columns if c not in numeric_cols]

    # -----------------------------------------------------
    # Imputers
    # -----------------------------------------------------
    num_imp = SimpleImputer(strategy="mean")
    cat_imp = SimpleImputer(strategy="most_frequent")

    X[numeric_cols] = num_imp.fit_transform(X[numeric_cols])
    X[categorical_cols] = cat_imp.fit_transform(X[categorical_cols])

    for c in categorical_cols:
        X[c] = X[c].astype("category")

    # -----------------------------------------------------
    # Train/Val split
    # -----------------------------------------------------
    strat = y if len(np.unique(y)) > 1 else None  # avoid stratify error
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, random_state=seed, stratify=strat
    )

    # -----------------------------------------------------
    # LightGBM Model
    # -----------------------------------------------------
    model = LGBMClassifier(
        n_estimators=300,
        learning_rate=0.03,
        num_leaves=64,
        n_jobs=-1,
        random_state=seed,
    )

    model.fit(X_train, y_train)

    val_acc = model.score(X_val, y_val)
    log(f"Seed {seed}: val accuracy = {val_acc:.4f}", logfile)

    # -----------------------------------------------------
    # Inference → submission.csv
    # -----------------------------------------------------
    test_csv = p / "test.csv"
    if test_csv.exists():
        df_test = pd.read_csv(test_csv)

        # Match missing columns
        for col in X.columns:
            if col not in df_test.columns:
                df_test[col] = np.nan

        # Ensure order is same as training
        df_test = df_test[X.columns]

        # Apply imputers
        df_test[numeric_cols] = num_imp.transform(df_test[numeric_cols])
        df_test[categorical_cols] = cat_imp.transform(df_test[categorical_cols])

        for c in categorical_cols:
            df_test[c] = df_test[c].astype("category")

        preds = model.predict(df_test)

        # Use id column if present
        if "id" in df_test.columns:
            submission = pd.DataFrame({"id": df_test["id"], label_col: preds})
        else:
            submission = pd.DataFrame({label_col: preds})

        submission.to_csv(output_path, index=False)
        log(f"Saved submission to {output_path}", logfile)
    else:
        log("No test.csv found → skipping submission generation", logfile)

    return val_acc

# -----------------------------
# Image pipeline
# -----------------------------

def run_image(dataset_dir, output_path, seed, logfile):
    if torch is None:
        log("Torch not installed → image pipeline disabled.", logfile)
        return None

    p = Path(dataset_dir)
    train_csv = p / "train.csv"
    test_csv = p / "test.csv"

    if not train_csv.exists():
        log("No train.csv found", logfile)
        return None

    # -----------------------------
    # Load CSVs
    # -----------------------------
    df = pd.read_csv(train_csv)
    log(f"Loaded train.csv shape={df.shape}", logfile)

    if 'image_name' not in df.columns or 'target' not in df.columns:
        log("Expected columns 'image_name' and 'target' not found.", logfile)
        return None

    # Train/val split
    X_train_df, X_val_df = train_test_split(
        df, test_size=0.2, random_state=seed, stratify=df['target']
    )

    # -----------------------------
    # Dataset class
    # -----------------------------
    class MDataset(torch.utils.data.Dataset):
        def __init__(self, df, img_dir, transform=None, is_test=False):
            self.df = df
            self.img_dir = Path(img_dir)
            self.transform = transform
            self.is_test = is_test

        def __len__(self):
            return len(self.df)

        def __getitem__(self, idx):
            img_name = str(self.df.iloc[idx]['image_name'])
            img_path = self.img_dir / img_name
            image = Image.open(img_path).convert("RGB")
            if self.transform:
                image = self.transform(image)
            if self.is_test:
                return image, img_name
            label = self.df.iloc[idx]['target']
            return image, torch.tensor(label, dtype=torch.float32)

    # -----------------------------
    # Transforms
    # -----------------------------
    train_transform = transforms.Compose([
        transforms.Resize((224,224)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomVerticalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])
    ])
    val_transform = transforms.Compose([
        transforms.Resize((224,224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])
    ])

    # -----------------------------
    # Datasets & loaders
    # -----------------------------
    train_dataset = MDataset(X_train_df, p/'train', transform=train_transform)
    val_dataset = MDataset(X_val_df, p/'train', transform=val_transform)

    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)

    device = "cuda" if torch.cuda.is_available() else "cpu"

    # -----------------------------
    # Model
    # -----------------------------
    model = models.resnet18(pretrained=True)
    model.fc = nn.Linear(model.fc.in_features, 1)  # binary classification
    model.to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    criterion = nn.BCEWithLogitsLoss()

    # -----------------------------
    # Training loop
    # -----------------------------
    best_acc = 0
    for epoch in range(3):  # small number for demonstration
        model.train()
        for imgs, labels in train_loader:
            imgs, labels = imgs.to(device), labels.to(device).unsqueeze(1)
            optimizer.zero_grad()
            outputs = model(imgs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

        # Validation
        model.eval()
        all_preds, all_labels = [], []
        with torch.no_grad():
            for imgs, labels in val_loader:
                imgs, labels = imgs.to(device), labels.to(device).unsqueeze(1)
                outputs = torch.sigmoid(model(imgs))
                all_preds.append(outputs.cpu())
                all_labels.append(labels.cpu())
        all_preds = torch.cat(all_preds)
        all_labels = torch.cat(all_labels)
        preds_bin = (all_preds > 0.5).float()
        val_acc = (preds_bin == all_labels).float().mean().item()
        log(f"Epoch {epoch+1}: val accuracy = {val_acc:.4f}", logfile)

    # -----------------------------
    # Test predictions
    # -----------------------------
    if test_csv.exists():
        df_test = pd.read_csv(test_csv)
        test_dataset = MDataset(df_test, p/'test', transform=val_transform, is_test=True)
        test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)

        model.eval()
        preds_list, ids_list = [], []
        with torch.no_grad():
            for imgs, img_names in test_loader:
                imgs = imgs.to(device)
                outputs = torch.sigmoid(model(imgs))
                preds_list.extend((outputs.cpu() > 0.5).int().numpy().flatten())
                ids_list.extend(img_names)

        submission = pd.DataFrame({"id": ids_list, "target": preds_list})
        submission.to_csv(output_path, index=False)
        log(f"Saved submission to {output_path}", logfile)

    return val_acc


# -----------------------------
# Placeholder timeseries pipeline
# -----------------------------
def run_timeseries(dataset_dir, output_path, seed, logfile):
    log("Time-series pipeline not implemented yet.", logfile)
    return None

# -----------------------------
# Unified pipeline dispatcher
# -----------------------------
def run_pipeline(dataset_dir, output_path, seed, logfile, modality):
    if modality == "text":
        return run_text(dataset_dir, output_path, seed, logfile)
    elif modality == "tabular":
        return run_tabular(dataset_dir, output_path, seed, logfile)
    elif modality == "image":
        return run_image(dataset_dir, output_path, seed, logfile)
    elif modality == "timeseries":
        return run_timeseries(dataset_dir, output_path, seed, logfile)
    else:
        log(f"Unknown modality. Skipping.", logfile)
        return None

# -----------------------------
# Main agent
# -----------------------------
def run_agent(dataset_dir, output_path, seeds, time_limit):
    logfile = "run_log.txt"
    with open(logfile, "w") as f:
        f.write("")

    modality = detect_modality(dataset_dir)
    log(f"Detected modality: {modality}", logfile)

    accs = []
    start = time.time()
    for s in seeds:
        if time.time() - start > time_limit:
            log("Time limit exceeded.", logfile)
            break
        acc = run_pipeline(dataset_dir, output_path, s, logfile, modality)
        if acc is not None:
            accs.append(acc)

    if len(accs) == 0:
        log("No valid seeds produced metrics.", logfile)
        return

    mean = float(np.mean(accs))
    se = float(np.std(accs, ddof=1) / np.sqrt(len(accs)))
    mean_pct = mean * 100
    se_pct = se * 100
    medal = f"{mean_pct:.2f} ± {se_pct:.2f}"

    log(f"Completed seeds. Mean={mean:.4f}, SE={se:.4f}", logfile)

    with open("metrics.json", "w") as f:
        json.dump({
            "mean": mean,
            "se": se,
            "Any Medal(%)": medal,
            "per_seed": accs
        }, f, indent=2)

    log("Wrote metrics.json and run_log.txt", logfile)

# -----------------------------
# CLI
# -----------------------------
import argparse

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset_dir", type=str, required=True)
    parser.add_argument("--output", type=str, required=True)
    parser.add_argument("--seeds", nargs="+", type=int, default=[0,1,2])
    parser.add_argument("--time_limit", type=int, default=86400)
    args = parser.parse_args()

    run_agent(
        dataset_dir=args.dataset_dir,
        output_path=args.output,
        seeds=args.seeds,
        time_limit=args.time_limit
    )